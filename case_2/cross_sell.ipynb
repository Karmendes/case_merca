{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.config('spark.driver.memory','8g').appName(\"cross_sell\").getOrCreate()\n",
    "from pyspark.sql.functions import col,struct,concat,collect_list,row_number\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType\n",
    "\n",
    "from os import listdir\n",
    "from functools import reduce"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformando os dados para parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RawToProcessed:\n",
    "    def __init__(self,spark,path_in,path_out):\n",
    "        self.path_in = path_in\n",
    "        self.path_out = path_out\n",
    "        self.spark = spark\n",
    "    def csv_to_parquet(self,filename_in,filename_out,**kwargs):\n",
    "        df = self.spark.read.csv(f'{self.path_in}/{filename_in}',**kwargs)\n",
    "        df.write.parquet(f'{self.path_out}/{filename_out}.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Caminhos e nomes de arquivos\n",
    "path_in = 'data/raw'\n",
    "path_out = 'data/processed'\n",
    "files = listdir(path_in)\n",
    "names = [file.split('.')[0] for file in files]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instanciando classe\n",
    "processor = RawToProcessed(spark,path_in,path_out)\n",
    "# Transformando cada arquivo para parquet\n",
    "[processor.csv_to_parquet(file,name,sep = ';',header = True) for file,name in zip(files,names)]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gerando Pares de produtos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lendo arquivos\n",
    "files = [filename for filename in listdir(\"data/processed/\") if filename.startswith('venda')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MakePairs:\n",
    "    def __init__(self,spark,path_in = 'data/processed/',path_out = 'data/gold/'):\n",
    "        self.path_in = path_in\n",
    "        self.path_out = path_out\n",
    "        self.spark = spark\n",
    "    def read_parquet(self,file):\n",
    "        # Lendo os dados\n",
    "        vendas = self.spark.read.parquet(f'data/processed/{file}')\n",
    "        vendas.cache()\n",
    "        return vendas\n",
    "    def transform(self,data):\n",
    "         # Agrupamento dos produtos por compra\n",
    "        grouped = data.groupBy(\"COD_ID_VENDA_UNICO\").agg(collect_list(\"COD_ID_PRODUTO\").alias(\"values\"))\n",
    "        # Gerando pares\n",
    "        pairs = (grouped.selectExpr(\"COD_ID_VENDA_UNICO\", \"explode(values) as value_col1\")\n",
    "                .join(grouped.selectExpr(\"COD_ID_VENDA_UNICO\", \"explode(values) as value_col2\"), \"COD_ID_VENDA_UNICO\"))\n",
    "        # Filtrando pares Ãºnicos e fazendo count\n",
    "        df_pairs_count = pairs.filter(col(\"value_col1\") < col(\"value_col2\")).groupby(\"value_col1\", \"value_col2\").count()\n",
    "        data.unpersist()\n",
    "        df_pairs_count.cache()\n",
    "        return df_pairs_count\n",
    "    def write_parquet(self,df_pairs_count,file):\n",
    "        df_pairs_count.write.parquet(f\"{self.path_out}{file}\")\n",
    "        df_pairs_count.unpersist()\n",
    "        print(f\"Arquivo {file} salvo\")\n",
    "    def do(self,file):\n",
    "        data = self.read_parquet(file)\n",
    "        df_pairs_count = self.transform(data)\n",
    "        self.write_parquet(df_pairs_count,file)\n",
    "        print(f\"Arquivo {file} salvo\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maker = MakePairs(spark)\n",
    "[maker.do(x) for x in files[:2]]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unindo dataframes, fazendo count geral e rankeando Top 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_from_months_rank_top_5(path = 'data/gold/vendas*.parquet/'):\n",
    "    # Gerando counts totais\n",
    "    vendas = spark.read.parquet(f'{path}')\n",
    "    vendas_count = vendas.groupBy(\"value_col1\",\"value_col2\").agg({\"count\":\"sum\"}).withColumnRenamed(\"sum(count)\", \"total\")\n",
    "    # Gerando Ranking dos pares pela contagem\n",
    "    w = Window.partitionBy(\"value_col1\").orderBy(col(\"total\").desc())\n",
    "    ranked_df = vendas_count.withColumn(\"rank\", row_number().over(w))\n",
    "    # Pegando apenas os 5 primeiros\n",
    "    top_5 = ranked_df.filter(col(\"rank\") <= 5)\n",
    "    top_5.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_from_months_rank_top_5()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spark_dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ffc2df374d79feb9e69dfdbdeeebbc1d0cea82c10f0fd84c08485d2f5b339a1b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
